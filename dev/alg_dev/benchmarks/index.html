<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Benchmark Suite · DiffEq Developer Documentation</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../index.html"><img class="logo" src="../../assets/logo.png" alt="DiffEq Developer Documentation logo"/></a><h1>DiffEq Developer Documentation</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><span class="toctext">Contributor Guide</span><ul><li><a class="toctext" href="../../contributing/ecosystem_overview/">Ecosystem Overview</a></li><li><a class="toctext" href="../../contributing/adding_packages/">Adding a new package to the common interface</a></li><li><a class="toctext" href="../../contributing/adding_algorithms/">Adding Algorithms</a></li><li><a class="toctext" href="../../contributing/defining_problems/">Developing A New Problem</a></li><li><a class="toctext" href="../../contributing/diffeq_internals/">The DiffEq Internals</a></li><li><a class="toctext" href="../../contributing/type_traits/">Type Traits</a></li></ul></li><li><span class="toctext">Algorithm Development Tools</span><ul><li><a class="toctext" href="../test_problems/">Test Problems</a></li><li><a class="toctext" href="../convergence/">Convergence Simulations</a></li><li class="current"><a class="toctext" href>Benchmark Suite</a><ul class="internal"></ul></li></ul></li><li><span class="toctext">Internal Documentation</span><ul><li><a class="toctext" href="../../internals/notes_on_algorithms/">Notes on Algorithms</a></li><li><a class="toctext" href="../../internals/tableaus/">ODE Tableaus</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Algorithm Development Tools</li><li><a href>Benchmark Suite</a></li></ul><a class="edit-page" href="https://github.com/JuliaDiffEq/DiffEqDevDocs.jl/blob/master/docs/src/alg_dev/benchmarks.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Benchmark Suite</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Benchmark-Suite-1" href="#Benchmark-Suite-1">Benchmark Suite</a></h1><p>DiffernetialEquations.jl provides a benchmarking suite to be able to test the difference in error, speed, and efficiency between algorithms. DifferentialEquations.jl includes current benchmarking notebooks to help users understand the performance of the methods. These benchmarking notebooks use the included benchmarking suite. There are two parts to the benchmarking suite: shootouts and work-precision. The <code>Shootout</code> tests methods head-to-head for timing and error on the same problem. A <code>WorkPrecision</code> draws a work-precision diagram for the algorithms in question on the chosen problem.</p><h3><a class="nav-anchor" id="Using-the-Benchmarking-Notebooks-1" href="#Using-the-Benchmarking-Notebooks-1">Using the Benchmarking Notebooks</a></h3><p>To use the benchmarking notebooks, IJulia is required. The commands are as follows:</p><pre><code class="language-julia">]add &quot;www.github.com/JuliaDiffEq/DiffEqBenchmarks&quot;
using IJulia
notebook(dir = Pkg.dir(&quot;DiffEqBenchmarks&quot;))</code></pre><h3><a class="nav-anchor" id="Shootout-1" href="#Shootout-1">Shootout</a></h3><p>A shootout is where you compare between algorithms. For example, to see how different Runge-Kutta algorithms fair against each other, one can define a setup which is a dictionary of Symbols to Any, where the symbol is the keyword argument. Then you call <code>Shootout</code> on that setup. The code is as follows:</p><pre><code class="language-julia">tspan = [0,10]
setups = [Dict(:alg=&gt;:DP5)
          Dict(:abstol=&gt;1e-3,:reltol=&gt;1e-6,:alg=&gt;:ode45) # Fix ODE to be normal
          Dict(:alg=&gt;:dopri5)]
prob = DifferentialEquations.prob_ode_large2Dlinear
names = [&quot;DifferentialEquations&quot;;&quot;ODE&quot;;&quot;ODEInterface&quot;]
shoot = Shootout(prob,tspan,setups;dt=1/2^(10),names=names)</code></pre><p>Note that keyword arguments applied to <code>Shootout</code> are applied to every run, so in this example every run has the same starting timestep.  Here we explicitly chose names. If you don&#39;t, then the algorithm name is the default. This returns a Shootout type which holds the times it took for each algorithm and the errors. Using these, it calculates the efficiency defined as 1/(error*time), i.e. if the error is low or the run was quick then it&#39;s efficient. <code>print(shoot)</code> will show all of this information, and <code>plot(shoot)</code> will show the efficiencies of the algorithms in comparison to each other.</p><p>For every benchmark function there is a special keyword <code>numruns</code> which controls the number of runs used in the time estimate. To be more precise, these functions by default run the algorithm 20 times on the problem and take the average time. This amount can be increased and decreased as needed.</p><p>The keyword <code>appxtrue</code> allows for specifying a reference against which the error is computed. The method of error computation can be specified by the keyword <code>error_estimate</code> with values <code>:L2</code> for the L2 error over the solution time interval, <code>:l2</code> calculates the l2 error at the actual steps and the default <code>:final</code> only compares the endpoints.</p><p>A ShootoutSet is a where you define a vector of probs and tspans and run a shootout on each of these values.</p><h3><a class="nav-anchor" id="WorkPrecision-1" href="#WorkPrecision-1">WorkPrecision</a></h3><p>A WorkPrecision calculates the necessary componnets of a work-precision plot. This shows how time scales with the user chosen tolerances on a given problem. To make a WorkPrecision, you give it a vector of absolute and relative tolerances:</p><pre><code class="language-julia">abstols = 1./10.^(3:10)
reltols = 1./10.^(3:10)
wp = WorkPrecision(prob,tspan,abstols,reltols;alg=:DP5,name=&quot;Dormand-Prince 4/5&quot;)</code></pre><p>If we want to plot many WorkPrecisions together in order to compare between algorithms, you can make a WorkPrecisionSet. To do so, you pass the setups into the function as well:</p><pre><code class="language-julia">wp_set = WorkPrecisionSet(prob,tspan,abstols,reltols,setups;dt=1/2^4,numruns=2)
setups = [Dict(:alg=&gt;:RK4);Dict(:alg=&gt;:Euler);Dict(:alg=&gt;:BS3);
          Dict(:alg=&gt;:Midpoint);Dict(:alg=&gt;:BS5);Dict(:alg=&gt;:DP5)]
wp_set = WorkPrecisionSet(prob,tspan,abstols,reltols,setups;dt=1/2^4,numruns=2)</code></pre><p>Both of these types have a plot recipe to produce a work-precision diagram, and a print which will show some relevant information.</p><footer><hr/><a class="previous" href="../convergence/"><span class="direction">Previous</span><span class="title">Convergence Simulations</span></a><a class="next" href="../../internals/notes_on_algorithms/"><span class="direction">Next</span><span class="title">Notes on Algorithms</span></a></footer></article></body></html>
